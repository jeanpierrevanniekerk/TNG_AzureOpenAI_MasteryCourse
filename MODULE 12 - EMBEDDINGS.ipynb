{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "077441dc-ff98-4e9f-84fa-7e848e642d0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48a12e83-a6c3-4c91-b3a0-9020d38b1d54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8eb73c8-6929-4cfd-98c8-1cabe151cdb4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Set up Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2268a501-28cf-4998-9252-e73c1ee43aa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1db763f5-1e27-4178-83f4-c4f10b5f5a17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-identity==1.6.0\r\n  Using cached azure_identity-1.6.0-py2.py3-none-any.whl (108 kB)\r\nCollecting streamlit==1.18.1\r\n  Using cached streamlit-1.18.1-py2.py3-none-any.whl (9.6 MB)\r\nCollecting openai==0.27.8\r\n  Using cached openai-0.27.8-py3-none-any.whl (73 kB)\r\nCollecting python-dotenv==0.21.0\r\n  Using cached python_dotenv-0.21.0-py3-none-any.whl (18 kB)\r\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.10/site-packages (from -r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 5)) (1.21.5)\r\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.10/site-packages (from -r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 6)) (1.4.4)\r\nCollecting matplotlib==3.6.3\r\n  Using cached matplotlib-3.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\r\nCollecting plotly==5.12.0\r\n  Using cached plotly-5.12.0-py2.py3-none-any.whl (15.2 MB)\r\nCollecting scipy==1.10.0\r\n  Using cached scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\r\nCollecting scikit-learn==1.2.0\r\n  Using cached scikit_learn-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\r\nRequirement already satisfied: tenacity in /databricks/python3/lib/python3.10/site-packages (from -r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 11)) (8.1.0)\r\nCollecting tiktoken==0.3.0\r\n  Using cached tiktoken-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\r\nCollecting llama-index==0.4.33\r\n  Using cached llama_index-0.4.33-py3-none-any.whl\r\nCollecting langchain==0.0.129\r\n  Using cached langchain-0.0.129-py3-none-any.whl (467 kB)\r\nCollecting faiss-cpu\r\n  Using cached faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\r\nRequirement already satisfied: azure-core<2.0.0,>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from azure-identity==1.6.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 1)) (1.27.1)\r\nCollecting msal-extensions~=0.3.0\r\n  Using cached msal_extensions-0.3.1-py2.py3-none-any.whl (18 kB)\r\nCollecting msal<2.0.0,>=1.7.0\r\n  Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB)\r\nRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from azure-identity==1.6.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 1)) (1.16.0)\r\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.10/site-packages (from azure-identity==1.6.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 1)) (37.0.1)\r\nRequirement already satisfied: cachetools>=4.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (4.2.4)\r\nRequirement already satisfied: tornado>=6.0.3 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (6.1)\r\nRequirement already satisfied: protobuf<4,>=3.12 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (3.19.4)\r\nCollecting pydeck>=0.1.dev5\r\n  Using cached pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\r\nCollecting altair>=3.2.0\r\n  Using cached altair-5.1.1-py3-none-any.whl (520 kB)\r\nCollecting validators>=0.2\r\n  Using cached validators-0.21.2-py3-none-any.whl (25 kB)\r\nRequirement already satisfied: gitpython!=3.1.19 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (3.1.27)\r\nRequirement already satisfied: typing-extensions>=3.10.0.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (4.3.0)\r\nRequirement already satisfied: packaging>=14.1 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (21.3)\r\nRequirement already satisfied: pyarrow>=4.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (8.0.0)\r\nCollecting watchdog\r\n  Using cached watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\r\nRequirement already satisfied: importlib-metadata>=1.4 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (4.11.3)\r\nRequirement already satisfied: click>=7.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (8.0.4)\r\nCollecting rich>=10.11.0\r\n  Using cached rich-13.5.2-py3-none-any.whl (239 kB)\r\nRequirement already satisfied: blinker>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (1.4)\r\nCollecting tzlocal>=1.1\r\n  Using cached tzlocal-5.0.1-py3-none-any.whl (20 kB)\r\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (2.8.2)\r\nRequirement already satisfied: requests>=2.4 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (2.28.1)\r\nCollecting semver\r\n  Using cached semver-3.0.1-py3-none-any.whl (17 kB)\r\nCollecting toml\r\n  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\r\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (9.2.0)\r\nCollecting pympler>=0.9\r\n  Using cached Pympler-1.0.1-py3-none-any.whl (164 kB)\r\nRequirement already satisfied: aiohttp in /databricks/python3/lib/python3.10/site-packages (from openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (3.8.4)\r\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.10/site-packages (from openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (4.64.1)\r\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib==3.6.3->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 7)) (4.25.0)\r\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib==3.6.3->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 7)) (3.0.9)\r\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.10/site-packages (from matplotlib==3.6.3->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 7)) (0.11.0)\r\nCollecting contourpy>=1.0.1\r\n  Using cached contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib==3.6.3->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 7)) (1.4.2)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn==1.2.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 10)) (2.2.0)\r\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn==1.2.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 10)) (1.2.0)\r\nRequirement already satisfied: regex>=2022.1.18 in /databricks/python3/lib/python3.10/site-packages (from tiktoken==0.3.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 12)) (2022.7.9)\r\nCollecting blobfile>=2\r\n  Using cached blobfile-2.0.2-py3-none-any.whl (74 kB)\r\nCollecting tenacity\r\n  Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\r\nRequirement already satisfied: dataclasses-json in /databricks/python3/lib/python3.10/site-packages (from llama-index==0.4.33->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 13)) (0.5.8)\r\nRequirement already satisfied: SQLAlchemy<2,>=1 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.129->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 14)) (1.4.39)\r\nRequirement already satisfied: PyYAML>=5.4.1 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.129->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 14)) (6.0)\r\nRequirement already satisfied: pydantic<2,>=1 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.129->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 14)) (1.10.6)\r\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 6)) (2022.1)\r\nRequirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (1.9.2)\r\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (4.0.2)\r\nRequirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (1.3.1)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (6.0.4)\r\nRequirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (1.3.3)\r\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (21.4.0)\r\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (2.0.4)\r\nRequirement already satisfied: jsonschema>=3.0 in /databricks/python3/lib/python3.10/site-packages (from altair>=3.2.0->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (4.16.0)\r\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.10/site-packages (from altair>=3.2.0->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (2.11.3)\r\nCollecting toolz\r\n  Using cached toolz-0.12.0-py3-none-any.whl (55 kB)\r\nCollecting pycryptodomex~=3.8\r\n  Using cached pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\r\nRequirement already satisfied: filelock~=3.0 in /databricks/python3/lib/python3.10/site-packages (from blobfile>=2->tiktoken==0.3.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 12)) (3.6.0)\r\nCollecting lxml~=4.9\r\n  Using cached lxml-4.9.3-cp310-cp310-manylinux_2_28_x86_64.whl (7.9 MB)\r\nRequirement already satisfied: urllib3<3,>=1.25.3 in /databricks/python3/lib/python3.10/site-packages (from blobfile>=2->tiktoken==0.3.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 12)) (1.26.11)\r\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.10/site-packages (from cryptography>=2.1.4->azure-identity==1.6.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 1)) (1.15.1)\r\nRequirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /databricks/python3/lib/python3.10/site-packages (from dataclasses-json->llama-index==0.4.33->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 13)) (1.5.1)\r\nRequirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /databricks/python3/lib/python3.10/site-packages (from dataclasses-json->llama-index==0.4.33->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 13)) (3.19.0)\r\nRequirement already satisfied: typing-inspect>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from dataclasses-json->llama-index==0.4.33->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 13)) (0.9.0)\r\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitpython!=3.1.19->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (4.0.10)\r\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata>=1.4->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (3.8.0)\r\nRequirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in /usr/lib/python3/dist-packages (from msal<2.0.0,>=1.7.0->azure-identity==1.6.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 1)) (2.3.0)\r\nCollecting portalocker<3,>=1.0\r\n  Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.4->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (2022.9.14)\r\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.4->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (3.3)\r\nCollecting pygments<3.0.0,>=2.13.0\r\n  Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB)\r\nCollecting markdown-it-py>=2.2.0\r\n  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\r\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.10/site-packages (from SQLAlchemy<2,>=1->langchain==0.0.129->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 14)) (1.1.1)\r\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-identity==1.6.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 1)) (2.21)\r\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (5.0.0)\r\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.10/site-packages (from jinja2->altair>=3.2.0->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (2.0.1)\r\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /databricks/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (0.18.0)\r\nCollecting mdurl~=0.1\r\n  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\r\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json->llama-index==0.4.33->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 13)) (0.4.3)\r\nInstalling collected packages: faiss-cpu, watchdog, validators, tzlocal, toolz, toml, tenacity, semver, scipy, python-dotenv, pympler, pygments, pycryptodomex, portalocker, mdurl, lxml, contourpy, scikit-learn, pydeck, plotly, matplotlib, markdown-it-py, blobfile, tiktoken, rich, openai, altair, streamlit, msal, msal-extensions, langchain, llama-index, azure-identity\r\n  Attempting uninstall: tenacity\r\n    Found existing installation: tenacity 8.1.0\r\n    Not uninstalling tenacity at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5644c1e1-5bb7-4ad0-aba9-1be9a5fd69fd\r\n    Can't uninstall 'tenacity'. No files were found to uninstall.\r\n  Attempting uninstall: scipy\r\n    Found existing installation: scipy 1.9.1\r\n    Not uninstalling scipy at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5644c1e1-5bb7-4ad0-aba9-1be9a5fd69fd\r\n    Can't uninstall 'scipy'. No files were found to uninstall.\r\n  Attempting uninstall: pygments\r\n    Found existing installation: Pygments 2.11.2\r\n    Not uninstalling pygments at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5644c1e1-5bb7-4ad0-aba9-1be9a5fd69fd\r\n    Can't uninstall 'Pygments'. No files were found to uninstall.\r\n  Attempting uninstall: scikit-learn\r\n    Found existing installation: scikit-learn 1.1.1\r\n    Not uninstalling scikit-learn at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5644c1e1-5bb7-4ad0-aba9-1be9a5fd69fd\r\n    Can't uninstall 'scikit-learn'. No files were found to uninstall.\r\n  Attempting uninstall: plotly\r\n    Found existing installation: plotly 5.9.0\r\n    Not uninstalling plotly at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5644c1e1-5bb7-4ad0-aba9-1be9a5fd69fd\r\n    Can't uninstall 'plotly'. No files were found to uninstall.\r\n  Attempting uninstall: matplotlib\r\n    Found existing installation: matplotlib 3.5.2\r\n    Not uninstalling matplotlib at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5644c1e1-5bb7-4ad0-aba9-1be9a5fd69fd\r\n    Can't uninstall 'matplotlib'. No files were found to uninstall.\r\n  Attempting uninstall: tiktoken\r\n    Found existing installation: tiktoken 0.4.0\r\n    Not uninstalling tiktoken at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5644c1e1-5bb7-4ad0-aba9-1be9a5fd69fd\r\n    Can't uninstall 'tiktoken'. No files were found to uninstall.\r\n  Attempting uninstall: openai\r\n    Found existing installation: openai 0.27.7\r\n    Not uninstalling openai at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5644c1e1-5bb7-4ad0-aba9-1be9a5fd69fd\r\n    Can't uninstall 'openai'. No files were found to uninstall.\r\n  Attempting uninstall: langchain\r\n    Found existing installation: langchain 0.0.181\r\n    Not uninstalling langchain at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5644c1e1-5bb7-4ad0-aba9-1be9a5fd69fd\r\n    Can't uninstall 'langchain'. No files were found to uninstall.\r\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ndatabricks-feature-store 0.13.5 requires pyspark<4,>=3.1.2, which is not installed.\r\nmleap 0.20.0 requires scikit-learn<0.23.0,>=0.22.0, but you have scikit-learn 1.2.0 which is incompatible.\u001B[0m\u001B[31m\r\n\u001B[0mSuccessfully installed altair-5.1.1 azure-identity-1.6.0 blobfile-2.0.2 contourpy-1.1.0 faiss-cpu-1.7.4 langchain-0.0.129 llama-index-0.4.33 lxml-4.9.3 markdown-it-py-3.0.0 matplotlib-3.6.3 mdurl-0.1.2 msal-1.23.0 msal-extensions-0.3.1 openai-0.27.8 plotly-5.12.0 portalocker-2.7.0 pycryptodomex-3.18.0 pydeck-0.8.1b0 pygments-2.16.1 pympler-1.0.1 python-dotenv-0.21.0 rich-13.5.2 scikit-learn-1.2.0 scipy-1.10.0 semver-3.0.1 streamlit-1.18.1 tenacity-8.2.3 tiktoken-0.3.0 toml-0.10.2 toolz-0.12.0 tzlocal-5.0.1 validators-0.21.2 watchdog-3.0.0\r\n\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5fe586-8100-48b2-b522-c3f9add41a17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up Azure OpenAI\n",
    "load_dotenv()\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32bf546d-44b3-41b7-8f40-96ca4c824838",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c5f1207-61b1-4e48-a040-c6744af28466",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [\"df\"], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# File location and type\n",
    "file_location = \"/FileStore/tables/bbc_news_data_embedding.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \"\\t\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format('com.databricks.spark.csv') \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"bbc_news_data_embedding_csv\"\n",
    "\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "df = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a50bf04-93d1-4204-bc15-348ff671d288",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Deploy a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e847861-e3d9-43ee-9803-a79766173b5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a succeeded deployment that supports embeddings with id: text-embedding-ada-002.\n"
     ]
    }
   ],
   "source": [
    "# list models deployed with embeddings capability\n",
    "deployment_id = None\n",
    "result = openai.Deployment.list()\n",
    "\n",
    "for deployment in result.data:\n",
    "    if deployment[\"status\"] != \"succeeded\":\n",
    "        continue\n",
    "    \n",
    "    model = openai.Model.retrieve(deployment[\"model\"])\n",
    "    if model[\"capabilities\"][\"embeddings\"] != True:\n",
    "        continue\n",
    "    \n",
    "    deployment_id = deployment[\"id\"]\n",
    "    break\n",
    "\n",
    "# if not model deployed, deploy one\n",
    "if not deployment_id:\n",
    "    print('No deployment with status: succeeded found.')\n",
    "    model = \"text-similarity-davinci-002\"\n",
    "\n",
    "    # Now let's create the deployment\n",
    "    print(f'Creating a new deployment with model: {model}')\n",
    "    result = openai.Deployment.create(model=model, scale_settings={\"scale_type\":\"standard\"})\n",
    "    deployment_id = result[\"id\"]\n",
    "    print(f'Successfully created {model} with deployment_id {deployment_id}')\n",
    "else:\n",
    "    print(f'Found a succeeded deployment that supports embeddings with id: {deployment_id}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5af8e1bd-3b65-4474-a7fe-e85dcacdeed1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Get Embeddings\n",
    "ref: https://learn.microsoft.com/en-us/azure/cognitive-services/openai/tutorials/embeddings?tabs=bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6751d915-82d9-4188-b5f9-1739e0084273",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n2225\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>embedding</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>\" Quarterly profits at US media giant TimeWarn...</td>\n",
       "      <td>[-0.0012276918860152364, 0.00733763724565506, ...</td>\n",
       "      <td>563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>\" The dollar has hit its highest level against...</td>\n",
       "      <td>[0.0009311728645116091, 0.014099937863647938, ...</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>\" The owners of embattled Russian oil giant Yu...</td>\n",
       "      <td>[-0.010487922467291355, 0.009665092453360558, ...</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>\" British Airways has blamed high fuel prices ...</td>\n",
       "      <td>[0.0111119095236063, 0.004624682944267988, -0....</td>\n",
       "      <td>558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "      <td>[-0.0021637482568621635, 0.005410161800682545,...</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>tech</td>\n",
       "      <td>397.txt</td>\n",
       "      <td>BT program to beat dialler scams</td>\n",
       "      <td>\" BT is introducing two initiatives to help be...</td>\n",
       "      <td>[0.007671569474041462, 0.00624304823577404, -0...</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>tech</td>\n",
       "      <td>398.txt</td>\n",
       "      <td>Spam e-mails tempt net shoppers</td>\n",
       "      <td>\" Computer users across the world continue to ...</td>\n",
       "      <td>[0.0026338498573750257, 0.015989987179636955, ...</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>tech</td>\n",
       "      <td>399.txt</td>\n",
       "      <td>Be careful how you code</td>\n",
       "      <td>\" A new European directive could put software ...</td>\n",
       "      <td>[0.007126151118427515, 0.008495588786900043, -...</td>\n",
       "      <td>1208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>tech</td>\n",
       "      <td>400.txt</td>\n",
       "      <td>US cyber security chief resigns</td>\n",
       "      <td>\" The man making sure US computer networks are...</td>\n",
       "      <td>[0.002447678940370679, 0.006076449993997812, -...</td>\n",
       "      <td>437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>tech</td>\n",
       "      <td>401.txt</td>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>\" Online role playing games are time-consuming...</td>\n",
       "      <td>None</td>\n",
       "      <td>3584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category  ... n_tokens\n",
       "0     business  ...      563\n",
       "1     business  ...      457\n",
       "2     business  ...      340\n",
       "3     business  ...      558\n",
       "4     business  ...      385\n",
       "...        ...  ...      ...\n",
       "2220      tech  ...      502\n",
       "2221      tech  ...      435\n",
       "2222      tech  ...     1208\n",
       "2223      tech  ...      437\n",
       "2224      tech  ...     3584\n",
       "\n",
       "[2225 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "df1 = df\n",
    "print(len(df1))\n",
    "df1['n_tokens'] = df1['content'].apply(lambda x: len(tokenizer.encode(x)))\n",
    "df1 = df1[df1.n_tokens<8192]\n",
    "print(len(df1))\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bf85a7f-22b7-42b3-8387-4de57b8eaccb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<command-1550531866917453>:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['embedding'][i] = embedding['data'][0]['embedding']\n"
     ]
    }
   ],
   "source": [
    "df['embedding'] = ''\n",
    "print(len(df))\n",
    "for i in range(len(df)):    \n",
    "    try:\n",
    "        embedding = openai.Embedding.create(input=df['content'][i], deployment_id=deployment_id)\n",
    "        df['embedding'][i] = embedding['data'][0]['embedding']\n",
    "    except Exception as err:\n",
    "        i\n",
    "        print(f\"Unexpected {err=}, {type(err)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7643377-df94-4603-83eb-c9c8192ccd5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS OpenAI\")\n",
    "df_sp = spark.createDataFrame(df)\n",
    "df_sp.write.mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('openai.document_analysis_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23527b2a-56d4-49be-9ebe-8be0124f027e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>embedding</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>\" Quarterly profits at US media giant TimeWarn...</td>\n",
       "      <td>[-0.01796060986816883, -0.01651429571211338, 0...</td>\n",
       "      <td>563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>\" The dollar has hit its highest level against...</td>\n",
       "      <td>[-0.021637994796037674, -0.012706875801086426,...</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>\" The owners of embattled Russian oil giant Yu...</td>\n",
       "      <td>[-0.01746140420436859, -0.03939249739050865, 0...</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>\" British Airways has blamed high fuel prices ...</td>\n",
       "      <td>[-0.019902830943465233, -0.017122669145464897,...</td>\n",
       "      <td>558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "      <td>[-0.008347506634891033, -0.008401579223573208,...</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>business</td>\n",
       "      <td>006.txt</td>\n",
       "      <td>Japan narrowly escapes recession</td>\n",
       "      <td>\" Japan's economy teetered on the brink of a t...</td>\n",
       "      <td>[-0.023057539016008377, -0.011685534380376339,...</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>business</td>\n",
       "      <td>007.txt</td>\n",
       "      <td>Jobs growth still slow in the US</td>\n",
       "      <td>\" The US created fewer jobs than expected in J...</td>\n",
       "      <td>[-0.04825970157980919, -0.019267477095127106, ...</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>business</td>\n",
       "      <td>008.txt</td>\n",
       "      <td>India calls for fair trade rules</td>\n",
       "      <td>\" India, which attends the G7 meeting of seven...</td>\n",
       "      <td>[-0.01497228816151619, -0.017229484394192696, ...</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>business</td>\n",
       "      <td>009.txt</td>\n",
       "      <td>Ethiopia's crop production up 24%</td>\n",
       "      <td>\" Ethiopia produced 14.27 million tonnes of cr...</td>\n",
       "      <td>[-0.007256364915519953, -0.0311115849763155, -...</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>business</td>\n",
       "      <td>010.txt</td>\n",
       "      <td>Court rejects $280bn tobacco case</td>\n",
       "      <td>A US government claim accusing the country's ...</td>\n",
       "      <td>[-0.008582530543208122, -0.00982870627194643, ...</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category  ... n_tokens\n",
       "0  business  ...      563\n",
       "1  business  ...      457\n",
       "2  business  ...      340\n",
       "3  business  ...      558\n",
       "4  business  ...      385\n",
       "5  business  ...      232\n",
       "6  business  ...      348\n",
       "7  business  ...      404\n",
       "8  business  ...      310\n",
       "9  business  ...      282\n",
       "\n",
       "[10 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = spark.sql('select * from openai.document_analysis_embeddings')\n",
    "sdf.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57070cb3-eb27-48ac-a2b0-40ce2cb922af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6352b48e-795c-4350-9f06-73a575b15c55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df.to_csv(\"../data/bbc-news-data-embedding.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de56e8d0-7f9b-4f6b-a51f-010ee9597647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MODULE 12 - EMBEDDINGS",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
