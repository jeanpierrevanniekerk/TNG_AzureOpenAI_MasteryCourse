{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f81624a4-7bf4-41cd-9db9-80667ce4add7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-identity==1.6.0\r\n  Downloading azure_identity-1.6.0-py2.py3-none-any.whl (108 kB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/108.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.0/108.5 kB\u001B[0m \u001B[31m1.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m108.5/108.5 kB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hCollecting streamlit==1.18.1\r\n  Downloading streamlit-1.18.1-py2.py3-none-any.whl (9.6 MB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/9.6 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/9.6 MB\u001B[0m \u001B[31m64.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m6.6/9.6 MB\u001B[0m \u001B[31m94.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m9.6/9.6 MB\u001B[0m \u001B[31m103.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m9.6/9.6 MB\u001B[0m \u001B[31m103.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.6/9.6 MB\u001B[0m \u001B[31m62.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hCollecting openai==0.27.8\r\n  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/73.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m73.6/73.6 kB\u001B[0m \u001B[31m8.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hCollecting python-dotenv==0.21.0\r\n  Downloading python_dotenv-0.21.0-py3-none-any.whl (18 kB)\r\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.10/site-packages (from -r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 5)) (1.21.5)\r\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.10/site-packages (from -r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 6)) (1.4.4)\r\nCollecting matplotlib==3.6.3\r\n  Downloading matplotlib-3.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/11.8 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/11.8 MB\u001B[0m \u001B[31m91.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m8.3/11.8 MB\u001B[0m \u001B[31m120.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m11.8/11.8 MB\u001B[0m \u001B[31m125.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m11.8/11.8 MB\u001B[0m \u001B[31m125.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.8/11.8 MB\u001B[0m \u001B[31m69.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hCollecting plotly==5.12.0\r\n  Downloading plotly-5.12.0-py2.py3-none-any.whl (15.2 MB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/15.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.6/15.2 MB\u001B[0m \u001B[31m198.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m11.5/15.2 MB\u001B[0m \u001B[31m176.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m13.8/15.2 MB\u001B[0m \u001B[31m12.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m15.2/15.2 MB\u001B[0m \u001B[31m12.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m15.2/15.2 MB\u001B[0m \u001B[31m12.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15.2/15.2 MB\u001B[0m \u001B[31m11.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hCollecting scipy==1.10.0\r\n  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/34.4 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.7/34.4 MB\u001B[0m \u001B[31m141.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.9/34.4 MB\u001B[0m \u001B[31m144.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/34.4 MB\u001B[0m \u001B[31m140.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.6/34.4 MB\u001B[0m \u001B[31m12.1 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m17.8/34.4 MB\u001B[0m \u001B[31m12.2 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m22.2/34.4 MB\u001B[0m \u001B[31m126.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━\u001B[0m \u001B[32m27.3/34.4 MB\u001B[0m \u001B[31m129.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━\u001B[0m \u001B[32m33.0/34.4 MB\u001B[0m \u001B[31m170.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m34.4/34.4 MB\u001B[0m \u001B[31m164.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m34.4/34.4 MB\u001B[0m \u001B[31m164.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m34.4/34.4 MB\u001B[0m \u001B[31m164.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m34.4/34.4 MB\u001B[0m \u001B[31m164.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m34.4/34.4 MB\u001B[0m \u001B[31m164.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m34.4/34.4 MB\u001B[0m \u001B[31m164.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m34.4/34.4 MB\u001B[0m \u001B[31m35.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hCollecting scikit-learn==1.2.0\r\n  Downloading scikit_learn-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/9.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m6.2/9.5 MB\u001B[0m \u001B[31m187.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m9.5/9.5 MB\u001B[0m \u001B[31m180.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m9.5/9.5 MB\u001B[0m \u001B[31m180.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.5/9.5 MB\u001B[0m \u001B[31m84.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hRequirement already satisfied: tenacity in /databricks/python3/lib/python3.10/site-packages (from -r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 11)) (8.1.0)\r\nCollecting tiktoken==0.3.0\r\n  Downloading tiktoken-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.6 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m69.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hCollecting llama-index==0.4.33\r\n  Downloading llama_index-0.4.33.tar.gz (147 kB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/147.2 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m147.2/147.2 kB\u001B[0m \u001B[31m14.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l-\b \bdone\r\n\u001B[?25hCollecting langchain==0.0.129\r\n  Downloading langchain-0.0.129-py3-none-any.whl (467 kB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/467.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m467.5/467.5 kB\u001B[0m \u001B[31m41.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hCollecting faiss-cpu\r\n  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/17.6 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/17.6 MB\u001B[0m \u001B[31m130.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.2/17.6 MB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.0/17.6 MB\u001B[0m \u001B[31m11.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m14.3/17.6 MB\u001B[0m \u001B[31m138.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m17.6/17.6 MB\u001B[0m \u001B[31m147.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m17.6/17.6 MB\u001B[0m \u001B[31m147.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m17.6/17.6 MB\u001B[0m \u001B[31m147.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m17.6/17.6 MB\u001B[0m \u001B[31m62.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from azure-identity==1.6.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 1)) (1.16.0)\r\nCollecting msal<2.0.0,>=1.7.0\r\n  Downloading msal-1.23.0-py2.py3-none-any.whl (90 kB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/90.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m90.8/90.8 kB\u001B[0m \u001B[31m13.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hCollecting msal-extensions~=0.3.0\r\n  Downloading msal_extensions-0.3.1-py2.py3-none-any.whl (18 kB)\r\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.10/site-packages (from azure-identity==1.6.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 1)) (37.0.1)\r\nRequirement already satisfied: azure-core<2.0.0,>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from azure-identity==1.6.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 1)) (1.27.1)\r\nRequirement already satisfied: blinker>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (1.4)\r\nRequirement already satisfied: gitpython!=3.1.19 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (3.1.27)\r\nRequirement already satisfied: typing-extensions>=3.10.0.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (4.3.0)\r\nRequirement already satisfied: pyarrow>=4.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (8.0.0)\r\nRequirement already satisfied: packaging>=14.1 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (21.3)\r\nCollecting altair>=3.2.0\r\n  Downloading altair-5.0.1-py3-none-any.whl (471 kB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/471.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m471.5/471.5 kB\u001B[0m \u001B[31m34.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (2.8.2)\r\nRequirement already satisfied: requests>=2.4 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (2.28.1)\r\nCollecting toml\r\n  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\r\nRequirement already satisfied: importlib-metadata>=1.4 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (4.11.3)\r\nRequirement already satisfied: cachetools>=4.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (4.2.4)\r\nCollecting tzlocal>=1.1\r\n  Downloading tzlocal-5.0.1-py3-none-any.whl (20 kB)\r\nRequirement already satisfied: click>=7.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (8.0.4)\r\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (9.2.0)\r\nRequirement already satisfied: protobuf<4,>=3.12 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (3.19.4)\r\nCollecting pympler>=0.9\r\n  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/164.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m164.8/164.8 kB\u001B[0m \u001B[31m15.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hRequirement already satisfied: tornado>=6.0.3 in /databricks/python3/lib/python3.10/site-packages (from streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (6.1)\r\nCollecting pydeck>=0.1.dev5\r\n  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/4.8 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m4.8/4.8 MB\u001B[0m \u001B[31m154.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.8/4.8 MB\u001B[0m \u001B[31m82.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hCollecting validators>=0.2\r\n  Downloading validators-0.20.0.tar.gz (30 kB)\r\n  Preparing metadata (setup.py) ... \u001B[?25l-\b \bdone\r\n\u001B[?25hCollecting watchdog\r\n  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/82.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m82.1/82.1 kB\u001B[0m \u001B[31m10.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hCollecting rich>=10.11.0\r\n  Downloading rich-13.5.2-py3-none-any.whl (239 kB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/239.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m239.7/239.7 kB\u001B[0m \u001B[31m26.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hCollecting semver\r\n  Downloading semver-3.0.1-py3-none-any.whl (17 kB)\r\nRequirement already satisfied: aiohttp in /databricks/python3/lib/python3.10/site-packages (from openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (3.8.4)\r\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.10/site-packages (from openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (4.64.1)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib==3.6.3->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 7)) (1.4.2)\r\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib==3.6.3->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 7)) (3.0.9)\r\nCollecting contourpy>=1.0.1\r\n  Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/300.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m300.7/300.7 kB\u001B[0m \u001B[31m33.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.10/site-packages (from matplotlib==3.6.3->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 7)) (0.11.0)\r\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib==3.6.3->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 7)) (4.25.0)\r\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn==1.2.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 10)) (1.2.0)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn==1.2.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 10)) (2.2.0)\r\nCollecting blobfile>=2\r\n  Downloading blobfile-2.0.2-py3-none-any.whl (74 kB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/74.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m74.5/74.5 kB\u001B[0m \u001B[31m10.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hRequirement already satisfied: regex>=2022.1.18 in /databricks/python3/lib/python3.10/site-packages (from tiktoken==0.3.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 12)) (2022.7.9)\r\nRequirement already satisfied: dataclasses_json in /databricks/python3/lib/python3.10/site-packages (from llama-index==0.4.33->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 13)) (0.5.8)\r\nCollecting tenacity\r\n  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\r\nRequirement already satisfied: SQLAlchemy<2,>=1 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.129->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 14)) (1.4.39)\r\nRequirement already satisfied: PyYAML>=5.4.1 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.129->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 14)) (6.0)\r\nRequirement already satisfied: pydantic<2,>=1 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.0.129->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 14)) (1.10.6)\r\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 6)) (2022.1)\r\nRequirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (1.3.3)\r\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (2.0.4)\r\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (21.4.0)\r\nRequirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (1.3.1)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (6.0.4)\r\nRequirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (1.9.2)\r\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->openai==0.27.8->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 3)) (4.0.2)\r\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.10/site-packages (from altair>=3.2.0->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (2.11.3)\r\nCollecting toolz\r\n  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/55.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.8/55.8 kB\u001B[0m \u001B[31m6.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hRequirement already satisfied: jsonschema>=3.0 in /databricks/python3/lib/python3.10/site-packages (from altair>=3.2.0->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (4.16.0)\r\nCollecting lxml~=4.9\r\n  Downloading lxml-4.9.3-cp310-cp310-manylinux_2_28_x86_64.whl (7.9 MB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/7.9 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/7.9 MB\u001B[0m \u001B[31m74.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.2/7.9 MB\u001B[0m \u001B[31m73.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m7.9/7.9 MB\u001B[0m \u001B[31m84.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.9/7.9 MB\u001B[0m \u001B[31m60.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hRequirement already satisfied: filelock~=3.0 in /databricks/python3/lib/python3.10/site-packages (from blobfile>=2->tiktoken==0.3.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 12)) (3.6.0)\r\nCollecting pycryptodomex~=3.8\r\n  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m88.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m47.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hRequirement already satisfied: urllib3<3,>=1.25.3 in /databricks/python3/lib/python3.10/site-packages (from blobfile>=2->tiktoken==0.3.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 12)) (1.26.11)\r\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.10/site-packages (from cryptography>=2.1.4->azure-identity==1.6.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 1)) (1.15.1)\r\nRequirement already satisfied: typing-inspect>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from dataclasses_json->llama-index==0.4.33->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 13)) (0.9.0)\r\nRequirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /databricks/python3/lib/python3.10/site-packages (from dataclasses_json->llama-index==0.4.33->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 13)) (3.19.0)\r\nRequirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /databricks/python3/lib/python3.10/site-packages (from dataclasses_json->llama-index==0.4.33->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 13)) (1.5.1)\r\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitpython!=3.1.19->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (4.0.10)\r\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata>=1.4->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (3.8.0)\r\nRequirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in /usr/lib/python3/dist-packages (from msal<2.0.0,>=1.7.0->azure-identity==1.6.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 1)) (2.3.0)\r\nCollecting portalocker<3,>=1.0\r\n  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\r\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.4->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (3.3)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.4->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (2022.9.14)\r\nCollecting markdown-it-py>=2.2.0\r\n  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/87.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m87.5/87.5 kB\u001B[0m \u001B[31m12.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hCollecting pygments<3.0.0,>=2.13.0\r\n  Downloading Pygments-2.15.1-py3-none-any.whl (1.1 MB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m55.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.10/site-packages (from SQLAlchemy<2,>=1->langchain==0.0.129->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 14)) (1.1.1)\r\nRequirement already satisfied: decorator>=3.4.0 in /databricks/python3/lib/python3.10/site-packages (from validators>=0.2->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (5.1.1)\r\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-identity==1.6.0->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 1)) (2.21)\r\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (5.0.0)\r\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.10/site-packages (from jinja2->altair>=3.2.0->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (2.0.1)\r\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /databricks/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.18.1->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 2)) (0.18.0)\r\nCollecting mdurl~=0.1\r\n  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\r\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses_json->llama-index==0.4.33->-r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt (line 13)) (0.4.3)\r\nBuilding wheels for collected packages: llama-index, validators\r\n  Building wheel for llama-index (setup.py) ... \u001B[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n\u001B[?25h  Created wheel for llama-index: filename=llama_index-0.4.33-py3-none-any.whl size=218530 sha256=f955c939c22e9eab9537f9fe2e5f6eebba7de4dd05a4162f3b8ecd110a5b4829\r\n  Stored in directory: /root/.cache/pip/wheels/c1/78/1f/72b73a862766efa77e2c11bc80a72d3d674a07f26389d4e7cc\r\n  Building wheel for validators (setup.py) ... \u001B[?25l-\b \b\\\b \b|\b \bdone\r\n\u001B[?25h  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19579 sha256=10fe3ff3adc6bca807759aa590b03dfb3fb928580350d135ad603ecfe2d2332b\r\n  Stored in directory: /root/.cache/pip/wheels/f2/ed/dd/d3a556ad245ef9dc570c6bcd2f22886d17b0b408dd3bbb9ac3\r\nSuccessfully built llama-index validators\r\nInstalling collected packages: faiss-cpu, watchdog, validators, tzlocal, toolz, toml, tenacity, semver, scipy, python-dotenv, pympler, pygments, pycryptodomex, portalocker, mdurl, lxml, contourpy, scikit-learn, pydeck, plotly, matplotlib, markdown-it-py, blobfile, tiktoken, rich, openai, altair, streamlit, msal, msal-extensions, langchain, llama-index, azure-identity\r\n  Attempting uninstall: tenacity\r\n    Found existing installation: tenacity 8.1.0\r\n    Not uninstalling tenacity at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c5da10d6-550b-4b3e-9012-a64f53f2a65e\r\n    Can't uninstall 'tenacity'. No files were found to uninstall.\r\n  Attempting uninstall: scipy\r\n    Found existing installation: scipy 1.9.1\r\n    Not uninstalling scipy at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c5da10d6-550b-4b3e-9012-a64f53f2a65e\r\n    Can't uninstall 'scipy'. No files were found to uninstall.\r\n  Attempting uninstall: pygments\r\n    Found existing installation: Pygments 2.11.2\r\n    Not uninstalling pygments at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c5da10d6-550b-4b3e-9012-a64f53f2a65e\r\n    Can't uninstall 'Pygments'. No files were found to uninstall.\r\n  Attempting uninstall: scikit-learn\r\n    Found existing installation: scikit-learn 1.1.1\r\n    Not uninstalling scikit-learn at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c5da10d6-550b-4b3e-9012-a64f53f2a65e\r\n    Can't uninstall 'scikit-learn'. No files were found to uninstall.\r\n  Attempting uninstall: plotly\r\n    Found existing installation: plotly 5.9.0\r\n    Not uninstalling plotly at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c5da10d6-550b-4b3e-9012-a64f53f2a65e\r\n    Can't uninstall 'plotly'. No files were found to uninstall.\r\n  Attempting uninstall: matplotlib\r\n    Found existing installation: matplotlib 3.5.2\r\n    Not uninstalling matplotlib at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c5da10d6-550b-4b3e-9012-a64f53f2a65e\r\n    Can't uninstall 'matplotlib'. No files were found to uninstall.\r\n  Attempting uninstall: tiktoken\r\n    Found existing installation: tiktoken 0.4.0\r\n    Not uninstalling tiktoken at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c5da10d6-550b-4b3e-9012-a64f53f2a65e\r\n    Can't uninstall 'tiktoken'. No files were found to uninstall.\r\n  Attempting uninstall: openai\r\n    Found existing installation: openai 0.27.7\r\n    Not uninstalling openai at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c5da10d6-550b-4b3e-9012-a64f53f2a65e\r\n    Can't uninstall 'openai'. No files were found to uninstall.\r\n  Attempting uninstall: langchain\r\n    Found existing installation: langchain 0.0.181\r\n    Not uninstalling langchain at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c5da10d6-550b-4b3e-9012-a64f53f2a65e\r\n    Can't uninstall 'langchain'. No files were found to uninstall.\r\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ndatabricks-feature-store 0.13.5 requires pyspark<4,>=3.1.2, which is not installed.\r\nmleap 0.20.0 requires scikit-learn<0.23.0,>=0.22.0, but you have scikit-learn 1.2.0 which is incompatible.\u001B[0m\u001B[31m\r\n\u001B[0mSuccessfully installed altair-5.0.1 azure-identity-1.6.0 blobfile-2.0.2 contourpy-1.1.0 faiss-cpu-1.7.4 langchain-0.0.129 llama-index-0.4.33 lxml-4.9.3 markdown-it-py-3.0.0 matplotlib-3.6.3 mdurl-0.1.2 msal-1.23.0 msal-extensions-0.3.1 openai-0.27.8 plotly-5.12.0 portalocker-2.7.0 pycryptodomex-3.18.0 pydeck-0.8.1b0 pygments-2.15.1 pympler-1.0.1 python-dotenv-0.21.0 rich-13.5.2 scikit-learn-1.2.0 scipy-1.10.0 semver-3.0.1 streamlit-1.18.1 tenacity-8.2.2 tiktoken-0.3.0 toml-0.10.2 toolz-0.12.0 tzlocal-5.0.1 validators-0.20.0 watchdog-3.0.0\r\n\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r /Workspace/Repos/jeanpierre.vanniekerk@truenorthgroup.co.za/document-analysis-using-gpt-3.ide/notebooks/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e4558b-9937-446e-b0aa-d78321411dec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (2.28.1)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests) (1.26.11)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests) (2022.9.14)\r\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests) (3.3)\r\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests) (2.0.4)\r\n\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\nRequirement already satisfied: pillow in /databricks/python3/lib/python3.10/site-packages (9.2.0)\r\n\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install pillow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c922e6-5e3e-4893-a21c-03580609c16c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up Azure OpenAI\n",
    "load_dotenv()\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaf39b0b-b00e-40ff-959f-54b95340443c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Redis Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f772283f-1dda-4fa8-a9cc-f8a76956b29c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting redis\r\n  Downloading redis-4.6.0-py3-none-any.whl (241 kB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/241.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.4/241.1 kB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m \u001B[32m235.5/241.1 kB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m241.1/241.1 kB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hRequirement already satisfied: async-timeout>=4.0.2 in /databricks/python3/lib/python3.10/site-packages (from redis) (4.0.2)\r\nInstalling collected packages: redis\r\nSuccessfully installed redis-4.6.0\r\n\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install redis\n",
    "!pip install redis-enterprise-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1684b24-66a5-4159-8a1c-9b9cdeca577b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Redis\nCreate embedding and save for entry  0  of  {'title': 'Building a chatbot in Azure that works with your data', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://blog.baeke.info/feed/', 'value': 'Building a chatbot in Azure that works with your data'}, 'links': [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://blog.baeke.info/2023/07/29/building-a-chatbot-based-on-your-documents-in-azure/'}], 'link': 'https://blog.baeke.info/2023/07/29/building-a-chatbot-based-on-your-documents-in-azure/', 'comments': 'https://blog.baeke.info/2023/07/29/building-a-chatbot-based-on-your-documents-in-azure/#respond', 'authors': [{'name': 'Geert Baeke'}], 'author': 'Geert Baeke', 'author_detail': {'name': 'Geert Baeke'}, 'published': 'Sat, 29 Jul 2023 20:41:55 +0000', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=29, tm_hour=20, tm_min=41, tm_sec=55, tm_wday=5, tm_yday=210, tm_isdst=0), 'tags': [{'term': 'ai', 'scheme': None, 'label': None}, {'term': 'azure', 'scheme': None, 'label': None}, {'term': 'chatgpt', 'scheme': None, 'label': None}, {'term': 'openai', 'scheme': None, 'label': None}], 'id': 'http://blog.baeke.info/?p=3922', 'guidislink': False, 'summary': 'When I talk to customers about Azure OpenAI, I am often asked how to build a chatbot that uses your own data in the simplest way possible while still allowing for some customization. In Azure, there are a few solutions. We will look at one of the solutions in this post. Note: don&#8217;t feel like &#8230; <a class=\"more-link\" href=\"https://blog.baeke.info/2023/07/29/building-a-chatbot-based-on-your-documents-in-azure/\">Continue reading<span class=\"screen-reader-text\"> \"Building a chatbot in Azure that works with your&#160;data\"</span></a>', 'summary_detail': {'type': 'text/html', 'language': None, 'base': 'https://blog.baeke.info/feed/', 'value': 'When I talk to customers about Azure OpenAI, I am often asked how to build a chatbot that uses your own data in the simplest way possible while still allowing for some customization. In Azure, there are a few solutions. We will look at one of the solutions in this post. Note: don&#8217;t feel like &#8230; <a class=\"more-link\" href=\"https://blog.baeke.info/2023/07/29/building-a-chatbot-based-on-your-documents-in-azure/\">Continue reading<span class=\"screen-reader-text\"> \"Building a chatbot in Azure that works with your&#160;data\"</span></a>'}, 'content': [{'type': 'text/html', 'language': None, 'base': 'https://blog.baeke.info/feed/', 'value': '<p>When I talk to customers about Azure OpenAI, I am often asked how to build a chatbot that uses your own data in the simplest way possible while still allowing for some customization. In Azure, there are a few solutions. We will look at one of the solutions in this post.</p>\\n\\n\\n\\n<p><strong>Note:</strong> don&#8217;t feel like reading? Check the video at the bottom of this post to see all this in action. </p>\\n\\n\\n\\n<p>The easiest solution is &#8220;<strong>Bring your own data&#8221;</strong>, also called <strong>&#8220;Azure OpenAI on your data&#8221;</strong>. See <a href=\"https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data\" rel=\"noreferrer noopener\" target=\"_blank\">Microsoft Learn</a> for more information. Right from Azure AI Studio, you can add data from a blob storage container or directly upload your data. In the end, the data ends up in Azure Cognitive Search, which is then linked to the Chat playground as shown in the image below:</p>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><a href=\"https://geertbaeke.files.wordpress.com/2023/07/image-7.png\"><img alt=\"\" class=\"wp-image-3927\" src=\"https://geertbaeke.files.wordpress.com/2023/07/image-7.png?w=1024\" /></a><figcaption class=\"wp-element-caption\">Bring your own data in Chat Playground (part of Azure AI Studio)</figcaption></figure>\\n\\n\\n\\n<p>The following file types are supported: txt, md, html, Word, PowerPoint, PDF. Above, I added a few PDF files with job descriptions. The standard upload procedure works well for small documents. For larger documents, you should use the <a href=\"https://github.com/microsoft/sample-app-aoai-chatGPT/tree/main/scripts\" rel=\"noreferrer noopener\" target=\"_blank\">data preparation</a> script. It will chunk documents into smaller pieces. The chunk size and overlap can be set in a <strong>config.json</strong> file. This is similar to what you can do with LangChain&#8217;s loaders and text splitters. In fact, you can use LangChain&#8217;s abstractions instead of the data preparation script but I have not tried this myself yet. See <a href=\"https://python.langchain.com/docs/integrations/vectorstores/azuresearch\" rel=\"noreferrer noopener\" target=\"_blank\">https://python.langchain.com/docs/integrations/vectorstores/azuresearch</a> for more information. If I find the time, I will report on my findings in another blog post.</p>\\n\\n\\n\\n<p>Right from the playground, you can click a button to deploy the bot to a new web app (Azure App Services):</p>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><a href=\"https://geertbaeke.files.wordpress.com/2023/07/image-8.png\"><img alt=\"\" class=\"wp-image-3929\" src=\"https://geertbaeke.files.wordpress.com/2023/07/image-8.png?w=1024\" /></a><figcaption class=\"wp-element-caption\">The same bot in an Azure Web App</figcaption></figure>\\n\\n\\n\\n<p>Although it is very easy to create the bot, there are a couple of things to note here:</p>\\n\\n\\n\\n<ul>\\n<li>The solution requires Azure Cognitive Search which is an extra cost. The minimum cost is around 70 euros per month. There are open-source solutions you can use for free or SaaS solutions that provide a free option (e.g., Pinecone). Azure OpenAI on your data only supports Azure Cognitive Search for now although technically, Microsoft could open this up to other stores.</li>\\n\\n\\n\\n<li>Azure Cognitive Search is somewhat more complex than (some) vector databases such as <a href=\"https://www.pinecone.io/\" rel=\"noreferrer noopener\" target=\"_blank\">Pinecone</a> or <a href=\"https://www.trychroma.com/\" rel=\"noreferrer noopener\" target=\"_blank\">Chroma</a>. If you want to use other search engines/vector databases, I recommend using <a href=\"https://python.langchain.com/docs/get_started/introduction.html\" rel=\"noreferrer noopener\" target=\"_blank\">LangChain</a> in combination with something like <a href=\"https://docs.chainlit.io/overview\" rel=\"noreferrer noopener\" target=\"_blank\">Chainlit</a> to create your prototype. Of course, that means you will have to write more code. No more wizards for you! <img alt=\"😃\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f603.png\" style=\"height: 1em;\" /></li>\\n\\n\\n\\n<li>The source code for the web app is at <a href=\"https://github.com/microsoft/sample-app-aoai-chatGPT\" rel=\"noreferrer noopener\" target=\"_blank\">https://github.com/microsoft/sample-app-aoai-chatGPT</a>. Although the code is not super complex, Python tools such as Streamlit and Chainlit make it much easier to create a prototype from scratch. Note that the web app is protected with Azure Active Directory by default and that it authenticates to Cognitive Search and Azure OpenAI using API keys set as environment variables. This is all automatically configured for you!</li>\\n\\n\\n\\n<li>Azure Cognitive Search integration is part of the Azure OpenAI API version 2023-06-01 and depends on a <strong>dataSources</strong> field in the JSON body sent to the Azure OpenAI API. Check the source code <a href=\"https://github.com/microsoft/sample-app-aoai-chatGPT/blob/main/app.py#L75\" rel=\"noreferrer noopener\" target=\"_blank\">here</a>. I would have preferred the API to stay aligned with the OpenAI APIs and retrieve extra content as a separate step.</li>\\n</ul>\\n\\n\\n\\n<p>With all this being said, if all you need for your demo is the web app generated by the Chat playground&#8217;s <strong>Deploy</strong> button, this is one of the quickest ways to get there!</p>\\n\\n\\n\\n<p>To see the entire experience in action, check out the video below or click this link: <a href=\"https://www.youtube.com/watch?v=gySeOggsz-w\" rel=\"noreferrer noopener\" target=\"_blank\">https://www.youtube.com/watch?v=gySeOggsz-w</a>.</p>\\n\\n\\n\\n<figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\\n\\n</div></figure>'}], 'wfw_commentrss': 'https://blog.baeke.info/2023/07/29/building-a-chatbot-based-on-your-documents-in-azure/feed/', 'slash_comments': '0', 'media_thumbnail': [{'url': 'https://geertbaeke.files.wordpress.com/2023/07/50fc8d64-fef4-4929-a484-4b09411e5bff.jpeg'}], 'href': '', 'media_content': [{'url': 'https://geertbaeke.files.wordpress.com/2023/07/50fc8d64-fef4-4929-a484-4b09411e5bff.jpeg', 'medium': 'image'}, {'url': 'https://0.gravatar.com/avatar/97885b85ba91ffe003519ed91a35af02eb2563b00a37278f33635aa1b10c1e60?s=96&d=identicon&r=G', 'medium': 'image'}, {'url': 'https://geertbaeke.files.wordpress.com/2023/07/image-7.png?w=1024', 'medium': 'image'}, {'url': 'https://geertbaeke.files.wordpress.com/2023/07/image-8.png?w=1024', 'medium': 'image'}]}\nCreate embedding and save for entry  1  of  {'title': 'Semantic Kernel Planner 101', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://blog.baeke.info/feed/', 'value': 'Semantic Kernel Planner 101'}, 'links': [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://blog.baeke.info/2023/06/01/semantic-kernel-planner-101/'}], 'link': 'https://blog.baeke.info/2023/06/01/semantic-kernel-planner-101/', 'comments': 'https://blog.baeke.info/2023/06/01/semantic-kernel-planner-101/#comments', 'authors': [{'name': 'Geert Baeke'}], 'author': 'Geert Baeke', 'author_detail': {'name': 'Geert Baeke'}, 'published': 'Thu, 01 Jun 2023 14:11:58 +0000', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=1, tm_hour=14, tm_min=11, tm_sec=58, tm_wday=3, tm_yday=152, tm_isdst=0), 'tags': [{'term': 'ai', 'scheme': None, 'label': None}, {'term': 'dalle', 'scheme': None, 'label': None}, {'term': 'llm', 'scheme': None, 'label': None}, {'term': 'openai', 'scheme': None, 'label': None}, {'term': 'semantickernel', 'scheme': None, 'label': None}, {'term': 'sk', 'scheme': None, 'label': None}], 'id': 'http://blog.baeke.info/?p=3858', 'guidislink': False, 'summary': 'Introduction If you are a developer who wants to build AI-first apps with natural language processing and large language models, you might be interested in Semantic Kernel (SK), a lightweight and open-source SDK that aims to simplify the integration of AI with conventional programming languages. SK is part of the CoPilot Stack and Microsoft is &#8230; <a class=\"more-link\" href=\"https://blog.baeke.info/2023/06/01/semantic-kernel-planner-101/\">Continue reading<span class=\"screen-reader-text\"> \"Semantic Kernel Planner&#160;101\"</span></a>', 'summary_detail': {'type': 'text/html', 'language': None, 'base': 'https://blog.baeke.info/feed/', 'value': 'Introduction If you are a developer who wants to build AI-first apps with natural language processing and large language models, you might be interested in Semantic Kernel (SK), a lightweight and open-source SDK that aims to simplify the integration of AI with conventional programming languages. SK is part of the CoPilot Stack and Microsoft is &#8230; <a class=\"more-link\" href=\"https://blog.baeke.info/2023/06/01/semantic-kernel-planner-101/\">Continue reading<span class=\"screen-reader-text\"> \"Semantic Kernel Planner&#160;101\"</span></a>'}, 'content': [{'type': 'text/html', 'language': None, 'base': 'https://blog.baeke.info/feed/', 'value': '<h2 class=\"wp-block-heading\">Introduction</h2>\\n\\n\\n\\n<p>If you are a developer who wants to build AI-first apps with natural language processing and large language models, you might be interested in Semantic Kernel (SK), a lightweight and open-source SDK that aims to simplify the integration of AI with conventional programming languages. </p>\\n\\n\\n\\n<p>SK is part of the CoPilot Stack and Microsoft is using it in its own CoPilots.</p>\\n\\n\\n\\n<p>SK allows you to create and orchestrate semantic functions, native functions, memories, and connectors using C# or Python. Much like <a href=\"https://python.langchain.com/en/latest/index.html\">LangChain</a>, it supports prompt templating, chaining, and memory with vectors (embeddings).</p>\\n\\n\\n\\n<p>You can also use SK’s planner to automatically generate and execute complex tasks based on a user’s goals. This is similar to LangChain&#8217;s <a href=\"https://python.langchain.com/en/latest/modules/agents/getting_started.html\">Agents &amp; Tools</a> capabilities. In this blog post, we will introduce some of the features and benefits of SK&#8217;s Planner, and show you how to use it in your own applications. I am still learning so I am going to stick to the basics! <img alt=\"😃\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f603.png\" style=\"height: 1em;\" /></p>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><a href=\"https://geertbaeke.files.wordpress.com/2023/05/image.png\"><img alt=\"\" class=\"wp-image-3860\" src=\"https://geertbaeke.files.wordpress.com/2023/05/image.png?w=998\" /></a><figcaption class=\"wp-element-caption\">Source: <a href=\"https://techcommunity.microsoft.com/t5/educator-developer-blog/unlock-the-potential-of-ai-in-your-apps-with-semantic-kernel-a/ba-p/3773847#:~:text=Semantic%20Kernel%20is%20designed%20to%20support%20and%20encapsulate,knowledge%20stores%20as%20well%20as%20your%20own%20data.\">Unlock the Potential of AI in Your Apps with Semantic Kernel: A Lightweight SDK for Large Language Models Integration (microsoft.com)</a></figcaption></figure>\\n\\n\\n\\n<p>SK&#8217;s Planner allows you to create and execute plans based on semantic queries. You start by providing it a goal (an ask). The goal could be: &#8220;Create a photo of a meal with these ingredients: {list of ingredients}&#8221;.  To achieve the goal, the planner can use plugins to generate and execute the plan. For the goal above, suppose we have two plugins:</p>\\n\\n\\n\\n<ul>\\n<li><strong>Recipe plugin</strong>: creates a recipe based on starter ingredients</li>\\n\\n\\n\\n<li><strong>Image description plugin</strong>: creates an image description based on any input</li>\\n</ul>\\n\\n\\n\\n<p>The recipe plugin takes a list of ingredients as input while the image description plugin can take the recipe as input and generate an image description of it. That image description could be used by DALL-E to generate an actual image.</p>\\n\\n\\n\\n<p><strong>Note:</strong> at the time of writing, Microsoft was on the verge of using the word plugin instead of skill. In the code, you will see references to skills but that will go away. This post already the word <strong>plugins</strong> instead of <strong>skills</strong>.</p>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">Creating plugins</h2>\\n\\n\\n\\n<p>Plugins make expertise available to SK and consist of one or more functions. A function can be either:</p>\\n\\n\\n\\n<ul>\\n<li>an LLM prompt: a <strong>semantic</strong> function</li>\\n\\n\\n\\n<li>native computer code: a <strong>native</strong> function</li>\\n</ul>\\n\\n\\n\\n<p>A plugin is a container where functions live. Think of it as a folder with each subfolder containing a function. For example:</p>\\n\\n\\n<div class=\"wp-block-image\">\\n<figure class=\"aligncenter size-large is-resized\"><a href=\"https://geertbaeke.files.wordpress.com/2023/05/image-1.png\"><img alt=\"\" class=\"wp-image-3864\" height=\"300\" src=\"https://geertbaeke.files.wordpress.com/2023/05/image-1.png?w=488\" width=\"366\" /></a><figcaption class=\"wp-element-caption\">Badly named <img alt=\"😃\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f603.png\" style=\"height: 1em;\" /> MySkills plugin with two semantic functions</figcaption></figure></div>\\n\\n\\n<p>A semantic function is a prompt with placeholders for one or more input variables. The prompt is in <strong>skprompt.txt</strong>. The Recipe function uses the following prompt:</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\">\\nWrite a recipe with the starter ingredients below and be specific about the steps to take and the amount of ingredients to use: \\n\\n{{$input}}\\n\\n</pre></div>\\n\\n\\n<p>The file <strong>config.json</strong> contains metadata about the function and LLM completion settings such as <a href=\"https://platform.openai.com/docs/api-reference/completions/create\">max_tokens</a> and <a href=\"https://platform.openai.com/docs/api-reference/completions/create\">temperature</a>. For example:</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: jscript; title: ; notranslate\">\\n{\\n    \"schema\": 1,\\n    \"type\": \"completion\",\\n    \"description\": \"Creates a recipe from starting ingredients\",\\n    \"completion\": {\\n        \"max_tokens\": 256,\\n        \"temperature\": 0,\\n        \"top_p\": 0,\\n        \"presence_penalty\": 0,\\n        \"frequency_penalty\": 0\\n    },\\n    \"input\": {\\n        \"parameters\": &#91;\\n            {\\n                \"name\": \"input\",\\n                \"description\": \"Input for this semantic function.\",\\n                \"defaultValue\": \"\"\\n            }\\n        ]\\n    },\\n    \"default_backends\": &#91;]\\n}\\n</pre></div>\\n\\n\\n<p>From your code, you can simply run this function to create a recipe. The plugin above is similar to a PromptTemplate in <a href=\"https://python.langchain.com/en/latest/index.html\">LangChain</a> that you can combine with an LLM into a <a href=\"https://python.langchain.com/en/latest/index.html\">chain</a>. You would then simply run the chain to get the output (a recipe). SK supports creating functions inline in your code as well, similar to how LangChain works.</p>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">Using the Planner</h2>\\n\\n\\n\\n<p>As stated above, the Planner can use plugins to reach the goal provided by a user&#8217;s ask. It actually works its way backward from the goal to create the plan:</p>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><a href=\"https://geertbaeke.files.wordpress.com/2023/05/image-2.png\"><img alt=\"\" class=\"wp-image-3870\" src=\"https://geertbaeke.files.wordpress.com/2023/05/image-2.png?w=719\" /></a><figcaption class=\"wp-element-caption\">Source: <a href=\"https://learn.microsoft.com/en-us/semantic-kernel/create-chains/planner\" rel=\"nofollow\">https://learn.microsoft.com/en-us/semantic-kernel/create-chains/planner</a></figcaption></figure>\\n\\n\\n\\n<p>There are different types of planners like a <strong>sequential planner</strong>, an <strong>action planner</strong>, a <strong>custom planner</strong>, and more. In our example, we will use a sequential planner and keep things as simple as possible. We will only use semantic functions, no native code functions.</p>\\n\\n\\n\\n<p>Time for some code. We will build a small .NET Console App based on the example above: create a recipe and generate a photo description for this recipe. Here is the code:</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: csharp; title: ; notranslate\">\\n\\nusing Microsoft.Extensions.Logging;\\nusing Microsoft.SemanticKernel;\\nusing Microsoft.SemanticKernel.AI.ImageGeneration;\\nusing System.Diagnostics;\\nusing Microsoft.SemanticKernel.Planning;\\nusing System.Text.Json;\\n\\nvar kernelSettings = KernelSettings.LoadSettings();\\n\\nvar kernelConfig = new KernelConfig();\\nkernelConfig.AddCompletionBackend(kernelSettings);\\n\\nusing ILoggerFactory loggerFactory = LoggerFactory.Create(builder =&gt;\\n{\\n    builder\\n        .SetMinimumLevel(kernelSettings.LogLevel ?? LogLevel.Warning)\\n        .AddConsole()\\n        .AddDebug();\\n});\\n\\nIKernel kernel = new KernelBuilder()\\n    .WithLogger(loggerFactory.CreateLogger&lt;IKernel&gt;())\\n    .WithConfiguration(kernelConfig)\\n    .Configure(c =&gt;\\n    {\\n        c.AddOpenAIImageGenerationService(kernelSettings.ApiKey);\\n\\n    })\\n    .Build();\\n\\n// used later to generate image with dallE\\nvar dallE = kernel.GetService&lt;IImageGeneration&gt;();\\n\\n// use SKs planner\\nvar planner = new SequentialPlanner(kernel);\\n\\n// depends on MySkills skill which has two semantic fucntions\\n// skills will be renamed to plugins in the future\\nvar skillsDirectory = Path.Combine(System.IO.Directory.GetCurrentDirectory(), &quot;skills&quot;);\\nvar skill = kernel.ImportSemanticSkillFromDirectory(skillsDirectory, &quot;MySkills&quot;);\\n\\n// ask for starter ingredients\\nConsole.Write(&quot;Enter ingredients: &quot;);\\nstring? input = Console.ReadLine();\\n\\n\\nif (!string.IsNullOrEmpty(input))\\n{\\n\\n    // define the ASK for the planner; the two semantic functions should be used by the plan\\n    // note that these functions are too simple to be useful in a real application\\n    // a single prompt to the model would be enough\\n    var ask = &quot;Create a photo of a meal with these ingredients:&quot; + input;\\n\\n    // create the plan and print it to see if the functions are used correctly\\n    var newPlan = await planner.CreatePlanAsync(ask);\\n\\n    Console.WriteLine(&quot;Updated plan:\\\\n&quot;);\\n    Console.WriteLine(JsonSerializer.Serialize(newPlan, new JsonSerializerOptions { WriteIndented = true }));\\n\\n    // run the plan; result should be an image description\\n    var newPlanResult = await newPlan.InvokeAsync();\\n\\n\\n    // generate the url to the images created by dalle\\n    Console.WriteLine(&quot;Plan result: &quot; + newPlanResult.ToString());\\n    var imageURL = await dallE.GenerateImageAsync(newPlanResult.ToString(), 512, 512);\\n\\n    // display image in browser (MacOS!!!)\\n    Process.Start(&quot;open&quot;, imageURL);\\n\\n</pre></div>\\n\\n\\n<p>The code uses <code>config/appsettings.json</code> which contains settings like serviceType, serviceId, and the API key to use with either OpenAI or Azure OpenAI. In my case, <code>serviceType</code> is <code>OpenAI</code> and the <code>serviceId</code> is <code>gpt-4</code>. Ensure you have <code>gpt-4</code> access in OpenAI&#8217;s API. I actually wanted to use Azure OpenAI but I do not have access to DALL-E and I do not think SK would support it anyway.</p>\\n\\n\\n\\n<p>After loading the settings, a <code>KernelConfig</code> is created, and a completion backend gets added (using <code>gpt-4</code> here). After setting up logging, a new kernel is created with a new KernelBuilder() with the following settings;</p>\\n\\n\\n\\n<ul>\\n<li>a logger</li>\\n\\n\\n\\n<li>the configuration with the completion backend</li>\\n\\n\\n\\n<li>an image generation service (DALL-E2 here) which needs the OpenAI API key (retrieved from <code>kernelSettings</code>)</li>\\n</ul>\\n\\n\\n\\n<p>We can now create the planner with <code>var planner = new SequentialPlanner(kernel);</code> and add skills (plugins) to the kernel. We add plugins from the <code>skills/MySkills</code> folder in our project.</p>\\n\\n\\n\\n<p>Now it&#8217;s just a matter of asking the user for some ingredients (stored in input) and creating the plan based on the ask. The ask is <strong>&#8220;Create a photo of a meal with these ingredients: &#8230;&#8221;</strong></p>\\n\\n\\n\\n<pre class=\"wp-block-code\"><code>var ask = \"Create a photo of a meal with these ingredients:\" + input;\\n\\nvar newPlan = await planner.CreatePlanAsync(ask);</code></pre>\\n\\n\\n\\n<p>Note that CreatePlanAsync does not execute the plan, it just creates it. We can look at the plan with the following code:</p>\\n\\n\\n\\n<pre class=\"wp-block-code\"><code>Console.WriteLine(\"Updated plan:\\\\n\");\\n    Console.WriteLine(JsonSerializer.Serialize(newPlan, new JsonSerializerOptions { WriteIndented = true }));</code></pre>\\n\\n\\n\\n<p>The output is something like this (note that there are typos in the ingredients but that&#8217;s ok, the model should understand):</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: jscript; title: ; notranslate\">\\n{\\n  \"state\": &#91;\\n    {\\n      \"Key\": \"INPUT\",\\n      \"Value\": \"\"\\n    }\\n  ],\\n  \"steps\": &#91;\\n    {\\n      \"state\": &#91;\\n        {\\n          \"Key\": \"INPUT\",\\n          \"Value\": \"\"\\n        }\\n      ],\\n      \"steps\": &#91;],\\n      \"parameters\": &#91;\\n        {\\n          \"Key\": \"INPUT\",\\n          \"Value\": \"courgette, collieflower, steak, tomato\"\\n        }\\n      ],\\n      \"outputs\": &#91;\\n        \"RECIPE_RESULT\"\\n      ],\\n      \"next_step_index\": 0,\\n      \"name\": \"Recipe\",\\n      \"skill_name\": \"MySkills\",\\n      \"description\": \"Creates a recipe from starting ingredients\"\\n    },\\n    {\\n      \"state\": &#91;\\n        {\\n          \"Key\": \"INPUT\",\\n          \"Value\": \"\"\\n        }\\n      ],\\n      \"steps\": &#91;],\\n      \"parameters\": &#91;\\n        {\\n          \"Key\": \"INPUT\",\\n          \"Value\": \"$RECIPE_RESULT\"\\n        }\\n      ],\\n      \"outputs\": &#91;\\n        \"RESULT__IMAGE_DESCRIPTION\"\\n      ],\\n      \"next_step_index\": 0,\\n      \"name\": \"ImageDesc\",\\n      \"skill_name\": \"MySkills\",\\n      \"description\": \"Generate image descript\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n    }\\n  }\\n}\\n</pre></div>\\n\\n\\n<p>In the <strong>build</strong> stanza, <strong>use &#8220;docker&#8221;</strong> tells Waypoint to build the container image from a local Dockerfile. With <strong>registry</strong>, we push that image to, in this case, Docker Hub. Instead of Docker Hub, other registries can be used as well. Before the image is pushed to the registry, it is first tagged with the tag you specify. Here, that is the <strong>latest</strong> tag. Although that is easy, you should not use that tag in your workflow because you will not get different images per application version. And you certainly want that when you do multiple deploys based on different code.</p>\\n\\n\\n\\n<p>To make the tag unique, you can replace &#8220;latest&#8221; with the <strong>gitrefpretty()</strong> function, as shown below:</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\">\\nbuild {\\n    use \"docker\" {}\\n    registry {\\n        use \"docker\" {\\n          image = \"gbaeke/wptest-hello\"\\n          tag = gitrefpretty()\\n          local = false\\n        }\\n    }\\n  }\\n</pre></div>\\n\\n\\n<p>Assuming you work with git and commit your code changes <img alt=\"😉\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f609.png\" style=\"height: 1em;\" />, gitrefpretty() will return the git commit sha at the time of build. </p>\\n\\n\\n\\n<p>You can check the commit sha of each commit with <strong>git log</strong>:</p>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><a href=\"https://geertbaeke.files.wordpress.com/2020/10/image.png\"><img alt=\"\" class=\"wp-image-2255\" src=\"https://geertbaeke.files.wordpress.com/2020/10/image.png?w=596\" /></a><figcaption>git log showing each commit with its sha-1 checksum</figcaption></figure>\\n\\n\\n\\n<p>When you use gitrefpretty() and you issue the <strong>waypoint build</strong> command, the images will be tagged with the sha-1 checksum. In Docker Hub, that is clearly shown:</p>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><a href=\"https://geertbaeke.files.wordpress.com/2020/10/image-1.png\"><img alt=\"\" class=\"wp-image-2257\" src=\"https://geertbaeke.files.wordpress.com/2020/10/image-1.png?w=1024\" /></a><figcaption>Image with commit sha tag pushed to Docker Hub</figcaption></figure>\\n\\n\\n\\n<p>That&#8217;s it for this quick post. If you have further questions, just hit me up on <a href=\"https://twitter.com/GeertBaeke\" rel=\"noreferrer noopener\" target=\"_blank\">Twitter </a>or leave a comment!</p>'}], 'wfw_commentrss': 'https://blog.baeke.info/2020/10/19/hashicorp-waypoint-image-tagging/feed/', 'slash_comments': '1', 'media_thumbnail': [{'url': 'https://geertbaeke.files.wordpress.com/2020/10/waypoint.jpg'}], 'href': '', 'media_content': [{'url': 'https://geertbaeke.files.wordpress.com/2020/10/waypoint.jpg', 'medium': 'image'}, {'url': 'https://0.gravatar.com/avatar/97885b85ba91ffe003519ed91a35af02eb2563b00a37278f33635aa1b10c1e60?s=96&d=identicon&r=G', 'medium': 'image'}, {'url': 'https://geertbaeke.files.wordpress.com/2020/10/image.png?w=596', 'medium': 'image'}, {'url': 'https://geertbaeke.files.wordpress.com/2020/10/image-1.png?w=1024', 'medium': 'image'}]}\nCreate embedding and save for entry  48  of  {'title': 'Azure Private Link and DNS', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://blog.baeke.info/feed/', 'value': 'Azure Private Link and DNS'}, 'links': [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://blog.baeke.info/2020/09/10/azure-private-link-and-dns/'}], 'link': 'https://blog.baeke.info/2020/09/10/azure-private-link-and-dns/', 'comments': 'https://blog.baeke.info/2020/09/10/azure-private-link-and-dns/#comments', 'authors': [{'name': 'Geert Baeke'}], 'author': 'Geert Baeke', 'author_detail': {'name': 'Geert Baeke'}, 'published': 'Thu, 10 Sep 2020 11:13:00 +0000', 'published_parsed': time.struct_time(tm_year=2020, tm_mon=9, tm_mday=10, tm_hour=11, tm_min=13, tm_sec=0, tm_wday=3, tm_yday=254, tm_isdst=0), 'tags': [{'term': 'Uncategorized', 'scheme': None, 'label': None}], 'id': 'http://blog.baeke.info/?p=2207', 'guidislink': False, 'summary': 'When you are just starting out with Azure Private Link, it can be hard figuring out how name resolution works and how DNS has to be configured. In this post, we will take a look at some of the internals and try to clear up some of the confusion. If you end up even more &#8230; <a class=\"more-link\" href=\"https://blog.baeke.info/2020/09/10/azure-private-link-and-dns/\">Continue reading<span class=\"screen-reader-text\"> \"Azure Private Link and&#160;DNS\"</span></a>', 'summary_detail': {'type': 'text/html', 'language': None, 'base': 'https://blog.baeke.info/feed/', 'value': 'When you are just starting out with Azure Private Link, it can be hard figuring out how name resolution works and how DNS has to be configured. In this post, we will take a look at some of the internals and try to clear up some of the confusion. If you end up even more &#8230; <a class=\"more-link\" href=\"https://blog.baeke.info/2020/09/10/azure-private-link-and-dns/\">Continue reading<span class=\"screen-reader-text\"> \"Azure Private Link and&#160;DNS\"</span></a>'}, 'content': [{'type': 'text/html', 'language': None, 'base': 'https://blog.baeke.info/feed/', 'value': '<p>When you are just starting out with Azure Private Link, it can be hard figuring out how name resolution works and how DNS has to be configured. In this post, we will take a look at some of the internals and try to clear up some of the confusion. If you end up even more confused then I&#8217;m sorry in advance. Drop me your questions in the comments if that happens. <img alt=\"😉\" class=\"wp-smiley\" src=\"https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f609.png\" style=\"height: 1em;\" /> I will illustrate the inner workings with a Cosmos DB account. It is similar for other services.</p>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">Wait! What is Private Link?</h2>\\n\\n\\n\\n<p><a href=\"https://azure.microsoft.com/en-us/services/private-link/\" rel=\"noreferrer noopener\" target=\"_blank\">Azure Private Link</a> provides private IP addresses for services such as Cosmos DB, Azure SQL Database and many more. You choose where the private IP address comes from by specifying a VNET and subnet. Without private link, these services are normally accessed via a public IP address or via <a href=\"https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-service-endpoints-overview\" rel=\"noreferrer noopener\" target=\"_blank\">Network Service Endpoints</a> (also the public IP but over the Azure network and restricted to selected subnets). There are several issues or shortcomings with those options:</p>\\n\\n\\n\\n<ul><li>for most customers, accessing databases and other services over the public Internet is just not acceptable</li><li>although network service endpoints provide a solution, this only works for systems that run inside an Azure Virtual Network (VNET)</li></ul>\\n\\n\\n\\n<p>When you want to access a service like Cosmos DB from on-premises networks and keep the traffic limited to your on-premises networks and Azure virtual networks, Azure Private Link is the way to go. In addition, you can filter the traffic with Azure Firewall or a virtual appliance, typically installed in a hub site. Now let&#8217;s take a look at how this works with Cosmos DB.</p>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">Azure Private Link for Cosmos DB</h2>\\n\\n\\n\\n<p>I deployed a Cosmos DB account in East US and called it <strong>geba-cosmos</strong>. To access this account and work with collections, I can use the following name: <a href=\"https://geba-cosmos.documents.azure.com:443/\" rel=\"nofollow\">https://geba-cosmos.documents.azure.com:443/</a>. As explained before, geba-cosmos.document.azure.com resolves to a public IP address.  Note that you can still control who can connect to this public IP address. Below, only my home IP address is allowed to connect:</p>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-2214\" src=\"https://geertbaeke.files.wordpress.com/2020/09/image.png?w=1024\" /><figcaption>Cosmos DB configured to allow access from selected networks</figcaption></figure>\\n\\n\\n\\n<p>In order to connect to Cosmos DB using a private IP address in your Azure Virtual Network, just click Private Endpoint Connections below Firewall and virtual networks:</p>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-2216\" src=\"https://geertbaeke.files.wordpress.com/2020/09/image-1.png?w=827\" /><figcaption>Private Endpoint Connections for a Cosmos DB account with one private endpoint configured</figcaption></figure>\\n\\n\\n\\n<p>To create a new private endpoint, click + Private Endpoint and follow the steps. The private endpoint is a resource on its own which needs a name and region. It should be in the same region as the virtual network you want to grab an IP address from. In the second screen, you can select the resource you want the private IP to point to (can be in a different region):</p>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-2218\" src=\"https://geertbaeke.files.wordpress.com/2020/09/image-2.png?w=777\" /><figcaption>Private endpoint that will connect to a Cosmos DB account in my directory (target sub-resource indicates the Cosmos DB API, here the Core SQL API is used)</figcaption></figure>\\n\\n\\n\\n<p>In the next step, you select the virtual network and subnet you want to grab an IP address from:</p>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-2220\" src=\"https://geertbaeke.files.wordpress.com/2020/09/image-3.png?w=760\" /><figcaption>VNET and subnet to grab the IP address for the private endpoint</figcaption></figure>\\n\\n\\n\\n<p>In this third step (Configuration), you will be asked if you want Private DNS integration. The default is <strong>Yes</strong> but I will select <strong>No</strong> for now.</p>\\n\\n\\n\\n<p><strong>Note:</strong> it is not required to use a Private DNS zone with Private Link</p>\\n\\n\\n\\n<p>When you finish the wizard and look at the created private endpoint, it will look similar to the screenshot below:</p>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-2223\" src=\"https://geertbaeke.files.wordpress.com/2020/09/image-5.png?w=1024\" /><figcaption>Private endpoint configured</figcaption></figure>\\n\\n\\n\\n<p>In the background, a network interface was created and attached to the selected virtual network. Above, the network interface is pe-geba-cosmos.nic.a755f7ad-9d54-4074-996c-8a14e9434898. The network interface screen will look like the screenshot below:</p>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-2225\" src=\"https://geertbaeke.files.wordpress.com/2020/09/image-6.png?w=1024\" /><figcaption>Network interface attached to subnet servers in VNET vnet-us1; it grabbed the next available IP of 10.1.0.5 as primary (but also 10.1.0.6 as secondary; click IP configurations to see that)</figcaption></figure>\\n\\n\\n\\n<p>The interesting part is the <strong>Custom DNS Settings</strong>. How can you resolve the name geba-cosmos.documents.azure.com to 10.1.0.5 when a client (either in Azure or on-premises) requests it? Let&#8217;s look at DNS resolution next&#8230;</p>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">DNS Resolution</h2>\\n\\n\\n\\n<p>Let&#8217;s use <a href=\"https://en.wikipedia.org/wiki/Dig_(command)\">dig</a> to check what a request for a Cosmos DB account return <strong>without</strong> private link. I have another account, geba-test, that I can use for that:</p>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-2227\" src=\"https://geertbaeke.files.wordpress.com/2020/09/image-7.png?w=800\" /><figcaption>dig with a Cosmos DB account without private link</figcaption></figure>\\n\\n\\n\\n<p>The above DNS request was made on my local machine, using public DNS servers. The response from Microsoft DNS servers for geba-test.documents.azure.com is a CNAME to a cloudapp.net name which results in IP address 40.78.226.8.</p>\\n\\n\\n\\n<p>The response from the DNS server will be different when private link is configured. When I resolve geba-cosmos.documents.azure.com, I get the following:</p>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-2229\" src=\"https://geertbaeke.files.wordpress.com/2020/09/image-8.png?w=856\" /><figcaption>Resolving the Cosmos DB hostname with private link configured</figcaption></figure>\\n\\n\\n\\n<p>As you can see, the Microsoft DNS servers respond with a CNAME of <strong>accountname.privatelink.documents.azure.com.</strong> but by default that CNAME goes to a cloudapp.net name that resolves to a public IP.</p>\\n\\n\\n\\n<p>This means that, if you don&#8217;t take specific action to resolve <strong>accountname.privatelink.documents.azure.com</strong> to the private IP, you will just end up with the public IP address. In most cases, you will not be able to connect because you will restrict public access to Cosmos DB. It&#8217;s important to note that you do not have to restrict public access and that you can enable both private and public access. Most customers I work with though, restrict public access.</p>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">Resolving to the private IP address</h2>\\n\\n\\n\\n<p>Before continuing, it&#8217;s important to state that developers should connect to <a href=\"https://accountname.documents.azure.com\" rel=\"nofollow\">https://accountname.documents.azure.com</a> (if they use the <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/performance-tips#networking\" rel=\"noreferrer noopener\" target=\"_blank\">gateway mode</a>). In fact, Cosmos DB expects you to use that name. Don&#8217;t try to connect with the IP address or some other name because it will not work. This is similar for services other than Cosmos DB. In the background though, we will make sure that accountname.documents.azure.com goes to the internal IP. So how do we make that happen? In what follows, I will list a couple of solutions. I will not discuss using a hosts file on your local pc, although it is possible to make that work. </p>\\n\\n\\n\\n<p><span style=\"text-decoration: underline;\">Create privatelink DNS zones on your DNS servers</span><br /><strong>Note:</strong> this approach is not recommended; it can present problems later when you need to connect to private link enabled services that are not under your control</p>\\n\\n\\n\\n<p>This means that in this case, we create a zone for privatelink.documents.azure.com on our own DNS servers and add the following records: </p>\\n\\n\\n\\n<ul><li>geba-cosmos.privatelink.documents.azure.com. IN A 10.1.0.5</li><li>geba-cosmos-eastus.privatelink.documents.azure.com. IN A 10.1.0.6</li></ul>\\n\\n\\n\\n<p><strong>Note:</strong> use a low TTL like 10s (similar to Azure Private DNS; see below)</p>\\n\\n\\n\\n<p>When the DNS server has to resolve geba-cosmos.documents.azure.com, it will get the CNAME response of geba-cosmos.privatelink.documents.azure.com and will be able to answer authoritatively that that is 10.1.0.5.</p>\\n\\n\\n\\n<p>If you use this solution, you need to make sure that you register the custom DNS settings listed by the private endpoint resource manually. If you want to try this yourself, you can easily do this with a Windows virtual machine with the DNS role or a Linux VM with bind.</p>\\n\\n\\n\\n<p><span style=\"text-decoration: underline;\">Use Azure Private DNS zones</span><br />If you do not want to register the custom DNS settings of the private endpoint manually in your own DNS servers, you can use <a href=\"https://docs.microsoft.com/en-us/azure/dns/private-dns-overview\" rel=\"noreferrer noopener\" target=\"_blank\">Azure Private DNS</a>. You can create the private DNS zone during the creation of the private endpoint. An internal zone for privatelink.documents.azure.com will be created and Azure will automatically add the required DNS configuration the private endpoint requires:</p>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-2232\" src=\"https://geertbaeke.files.wordpress.com/2020/09/image-9.png?w=1024\" /><figcaption>Azure Private DNS with automatic registration of the required Cosmos DB A records</figcaption></figure>\\n\\n\\n\\n<p>This is great for systems running in Azure virtual networks that are associated with the private DNS zone and that use the DNS servers provided by Azure but you still need to integrate your on-premises DNS servers with these private DNS zones. The way to do that is explained in the <a href=\"https://docs.microsoft.com/en-us/azure/private-link/private-endpoint-dns#on-premises-workloads-using-a-dns-forwarder\" rel=\"noreferrer noopener\" target=\"_blank\">documentation</a>. In particular, the below diagram is important:</p>\\n\\n\\n\\n<figure class=\"wp-block-image\"><img alt=\"On-premises forwarding to Azure DNS\" src=\"https://docs.microsoft.com/en-us/azure/private-link/media/private-endpoint-dns/on-premises-forwarding-to-azure.png\" /><figcaption>Source: Microsoft docs</figcaption></figure>\\n\\n\\n\\n<p>The example above is for Azure SQL Database but it is similar to our Cosmos DB example. In essence, you need the following:</p>\\n\\n\\n\\n<ul><li>DNS forwarder in the VNET (above, that is 10.5.0.254): this is an <strong>extra </strong>(!!!) Windows or Linux VM configured as a DNS forwarder; it should forward to <strong>168.63.129.16</strong> which points to the Azure-provided DNS servers; if the virtual network of the VM is integrated with the private DNS zone that hosts privatelink.documents.azure.com, the A records in that zone can be resolved properly</li><li>To allow the on-premises server to return the privatelink A records, setup conditional forwarding for documents.azure.com to the DNS forwarder in the virtual network</li></ul>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">What should you do?</h2>\\n\\n\\n\\n<p>That&#8217;s always difficult to answer but most customers I work with initially tend to go for option 1. They want to create a zone for privatelink.x.y.z and register the records manually. Although that could be automated, it&#8217;s often a manual step. In general, I do not recommend using it.</p>\\n\\n\\n\\n<p>I prefer the private DNS method because of the automatic registration of the records and the use of conditional forwarding. Although I don&#8217;t like the extra DNS servers, they will not be needed most of the time because customers tend to work with the hub/spoke model and the hub already contains DNS servers. Those DNS servers can then be configured to enable the resolution of the privatelink zones via forwarding.</p>'}], 'wfw_commentrss': 'https://blog.baeke.info/2020/09/10/azure-private-link-and-dns/feed/', 'slash_comments': '10', 'media_content': [{'url': 'https://0.gravatar.com/avatar/97885b85ba91ffe003519ed91a35af02eb2563b00a37278f33635aa1b10c1e60?s=96&d=identicon&r=G', 'medium': 'image'}, {'url': 'https://geertbaeke.files.wordpress.com/2020/09/image.png?w=1024', 'medium': 'image'}, {'url': 'https://geertbaeke.files.wordpress.com/2020/09/image-1.png?w=827', 'medium': 'image'}, {'url': 'https://geertbaeke.files.wordpress.com/2020/09/image-2.png?w=777', 'medium': 'image'}, {'url': 'https://geertbaeke.files.wordpress.com/2020/09/image-3.png?w=760', 'medium': 'image'}, {'url': 'https://geertbaeke.files.wordpress.com/2020/09/image-5.png?w=1024', 'medium': 'image'}, {'url': 'https://geertbaeke.files.wordpress.com/2020/09/image-6.png?w=1024', 'medium': 'image'}, {'url': 'https://geertbaeke.files.wordpress.com/2020/09/image-7.png?w=800', 'medium': 'image'}, {'url': 'https://geertbaeke.files.wordpress.com/2020/09/image-8.png?w=856', 'medium': 'image'}, {'url': 'https://geertbaeke.files.wordpress.com/2020/09/image-9.png?w=1024', 'medium': 'image'}, {'url': 'https://docs.microsoft.com/en-us/azure/private-link/media/private-endpoint-dns/on-premises-forwarding-to-azure.png', 'medium': 'image'}]}\nCreate embedding and save for entry  49  of  {'title': 'How to delete an stubborn Azure Virtual Hub', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://blog.baeke.info/feed/', 'value': 'How to delete an stubborn Azure Virtual Hub'}, 'links': [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://blog.baeke.info/2020/08/26/how-to-delete-an-stubborn-azure-virtual-hub/'}], 'link': 'https://blog.baeke.info/2020/08/26/how-to-delete-an-stubborn-azure-virtual-hub/', 'comments': 'https://blog.baeke.info/2020/08/26/how-to-delete-an-stubborn-azure-virtual-hub/#respond', 'authors': [{'name': 'Geert Baeke'}], 'author': 'Geert Baeke', 'author_detail': {'name': 'Geert Baeke'}, 'published': 'Wed, 26 Aug 2020 12:49:55 +0000', 'published_parsed': time.struct_time(tm_year=2020, tm_mon=8, tm_mday=26, tm_hour=12, tm_min=49, tm_sec=55, tm_wday=2, tm_yday=239, tm_isdst=0), 'tags': [{'term': 'azure', 'scheme': None, 'label': None}, {'term': 'hub', 'scheme': None, 'label': None}, {'term': 'network', 'scheme': None, 'label': None}, {'term': 'routing', 'scheme': None, 'label': None}, {'term': 'spoke', 'scheme': None, 'label': None}], 'id': 'http://blog.baeke.info/?p=2197', 'guidislink': False, 'summary': 'A while ago, I created an Azure Virtual WAN (Standard) and added a virtual hub. For some reason, the virtual hub ended up in the state below: I tried to reset the router and virtual hub but to no avail. Next, I tried to delete the hub. In the portal, this resulted in a validating &#8230; <a class=\"more-link\" href=\"https://blog.baeke.info/2020/08/26/how-to-delete-an-stubborn-azure-virtual-hub/\">Continue reading<span class=\"screen-reader-text\"> \"How to delete an stubborn Azure Virtual&#160;Hub\"</span></a>', 'summary_detail': {'type': 'text/html', 'language': None, 'base': 'https://blog.baeke.info/feed/', 'value': 'A while ago, I created an Azure Virtual WAN (Standard) and added a virtual hub. For some reason, the virtual hub ended up in the state below: I tried to reset the router and virtual hub but to no avail. Next, I tried to delete the hub. In the portal, this resulted in a validating &#8230; <a class=\"more-link\" href=\"https://blog.baeke.info/2020/08/26/how-to-delete-an-stubborn-azure-virtual-hub/\">Continue reading<span class=\"screen-reader-text\"> \"How to delete an stubborn Azure Virtual&#160;Hub\"</span></a>'}, 'content': [{'type': 'text/html', 'language': None, 'base': 'https://blog.baeke.info/feed/', 'value': '<p>A while ago, I created an <a href=\"https://docs.microsoft.com/en-us/azure/virtual-wan/virtual-wan-about\">Azure Virtual WAN</a> (Standard) and added a virtual hub. For some reason, the virtual hub ended up in the state below:</p>\\n\\n\\n\\n<figure class=\"wp-block-image\"><img alt=\"Image\" src=\"https://pbs.twimg.com/media/Ef2g33tWsAAhkP-?format=png&amp;name=small\" /><figcaption>Hub and routing status: Failed (Ouch!)</figcaption></figure>\\n\\n\\n\\n<p>I tried to reset the router and virtual hub but to no avail. Next, I tried to delete the hub. In the portal, this resulted in a validating state that did not end. In the Azure CLI, an error was thrown.</p>\\n\\n\\n\\n<p>Because this is a Microsoft Partner Network (MPN) subscription, I also did not have technical support or an easy way to enable it. I ended up buying Developer Support for a month just to open a service request.</p>\\n\\n\\n\\n<p>The (helpful) support engineer asked me to do the following:</p>\\n\\n\\n\\n<ul><li>Open <a href=\"https://resources.azure.com/\" rel=\"noreferrer noopener\" target=\"_blank\">Azure Resource Explorer</a></li><li>Navigate to subscriptions and the resource group that contains the hub</li><li>Under <strong>providers</strong>, navigate to Microsoft.Network</li><li>Locate the virtual hub and do GET, EDIT, PUT (set Read/Write mode first)</li></ul>\\n\\n\\n\\n<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-2203\" src=\"https://geertbaeke.files.wordpress.com/2020/08/image-12.png?w=315\" /><figcaption>After clicking GET and EDIT, PUT can be clicked</figcaption></figure>\\n\\n\\n\\n<p>At first it did not seem to work but in my case, the PUT operation just took a very long time. After the PUT operation finished, I could delete the virtual hub from the portal.</p>\\n\\n\\n\\n<p>Long story short: if you ever have a resource you cannot delete, give Azure Resource Explorer and the above procedure a try. Your mileage may vary though!</p>'}], 'wfw_commentrss': 'https://blog.baeke.info/2020/08/26/how-to-delete-an-stubborn-azure-virtual-hub/feed/', 'slash_comments': '0', 'media_thumbnail': [{'url': 'https://geertbaeke.files.wordpress.com/2020/08/image-12.png'}], 'href': '', 'media_content': [{'url': 'https://geertbaeke.files.wordpress.com/2020/08/image-12.png', 'medium': 'image'}, {'url': 'https://0.gravatar.com/avatar/97885b85ba91ffe003519ed91a35af02eb2563b00a37278f33635aa1b10c1e60?s=96&d=identicon&r=G', 'medium': 'image'}, {'url': 'https://pbs.twimg.com/media/Ef2g33tWsAAhkP-?format=png&name=small', 'medium': 'image'}, {'url': 'https://geertbaeke.files.wordpress.com/2020/08/image-12.png?w=315', 'medium': 'image'}]}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import redis\n",
    "import openai\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import numpy as np \n",
    " \n",
    "# OpenAI API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    " \n",
    "# Redis connection details\n",
    "redis_host = os.getenv('REDIS_HOST')\n",
    "redis_port = '10000'\n",
    "redis_password = os.getenv('REDIS_PASSWORD')\n",
    " \n",
    "# Connect to the Redis server\n",
    "conn = redis.Redis(host=redis_host, port=redis_port, password=redis_password)#, encoding='utf-8', decode_responses=True)\n",
    "if conn.ping():\n",
    "    print(\"Connected to Redis\")\n",
    " \n",
    "# URL of the RSS feed to parse\n",
    "url = 'https://blog.baeke.info/feed/'\n",
    " \n",
    "# Parse the RSS feed with feedparser\n",
    "feed = feedparser.parse(url)\n",
    " \n",
    "p = conn.pipeline(transaction=False)\n",
    "for i, entry in enumerate(feed.entries[:50]):\n",
    "    # report progress\n",
    "    print(\"Create embedding and save for entry \", i, \" of \", entry)\n",
    " \n",
    "    r = requests.get(entry.link)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    article = soup.find('div', {'class': 'entry-content'}).text\n",
    " \n",
    "    #vectorize with OpenAI text-emebdding-ada-002\n",
    "    embedding = openai.Embedding.create(input=article,model=\"text-embedding-ada-002\")\n",
    "\n",
    "    # print the embedding (length = 1536)\n",
    "    vector = embedding[\"data\"][0][\"embedding\"]\n",
    " \n",
    "    # convert to numpy array and bytes\n",
    "    vector = np.array(vector).astype(np.float32).tobytes()\n",
    " \n",
    "    # Create a new hash with url and embedding\n",
    "    post_hash = {\"url\": entry.link,\"embedding\": vector}\n",
    " \n",
    "    # create hash\n",
    "    conn.hset(name=f\"post:{i}\", mapping=post_hash)\n",
    "p.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "954e6434-57a6-4cee-bd26-53b616427890",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index already exists\n"
     ]
    }
   ],
   "source": [
    "#To create an index with Python code, check the code below:\n",
    "import redis\n",
    "from redis.commands.search.field import VectorField, TextField\n",
    "from redis.commands.search.query import Query\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    " \n",
    "# Redis connection details\n",
    "redis_host = os.getenv('REDIS_HOST')\n",
    "redis_port = os.getenv('REDIS_PORT')\n",
    "redis_password = os.getenv('REDIS_PASSWORD')\n",
    " \n",
    "# Connect to the Redis server\n",
    "conn = redis.Redis(host=redis_host, port=redis_port, password=redis_password, encoding='utf-8', decode_responses=True)\n",
    " \n",
    "SCHEMA = [TextField(\"url\"),VectorField(\"embedding\", \"HNSW\", {\"TYPE\": \"FLOAT32\", \"DIM\": 1536, \"DISTANCE_METRIC\": \"COSINE\"})]\n",
    " \n",
    "# Create the index\n",
    "try:\n",
    "    conn.ft(\"posts\").create_index(fields=SCHEMA, definition=IndexDefinition(prefix=[\"post:\"], index_type=IndexType.HASH))\n",
    "except Exception as e:\n",
    "    print(\"Index already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a877812-09e5-4161-8f9f-3eec20b7a937",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Redis\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Enter your query:  bot"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing query...\nSearching for similar posts...\nFound 0 results:\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from redis.commands.search.query import Query\n",
    "import redis\n",
    "import openai\n",
    "import os\n",
    " \n",
    "def search_vectors(query_vector, client, top_k=5):\n",
    "    base_query = \"*=>[KNN 5 @embedding $vector AS vector_score]\"\n",
    "    query = Query(base_query).return_fields(\"url\", \"vector_score\").sort_by(\"vector_score\").dialect(2)    \n",
    " \n",
    "    try:\n",
    "        results = client.ft(\"posts\").search(query, query_params={\"vector\": query_vector})\n",
    "    except Exception as e:\n",
    "        print(\"Error calling Redis search: \", e)\n",
    "        return None\n",
    " \n",
    "    return results\n",
    " \n",
    "# Connect to the Redis server\n",
    "conn = redis.Redis(host=redis_host, port=redis_port, password=redis_password, encoding='utf-8', decode_responses=True)\n",
    " \n",
    "if conn.ping():\n",
    "    print(\"Connected to Redis\")\n",
    " \n",
    "# Enter a query\n",
    "query = input(\"Enter your query: \")\n",
    " \n",
    "# Vectorize the query using OpenAI's text-embedding-ada-002 model\n",
    "print(\"Vectorizing query...\")\n",
    "embedding = openai.Embedding.create(input=query, model=\"text-embedding-ada-002\")\n",
    "query_vector = embedding[\"data\"][0][\"embedding\"]\n",
    " \n",
    "# Convert the vector to a numpy array\n",
    "query_vector = np.array(query_vector).astype(np.float32).tobytes()\n",
    " \n",
    "# Perform the similarity search\n",
    "print(\"Searching for similar posts...\")\n",
    "results = search_vectors(query_vector, conn)\n",
    " \n",
    "if results:\n",
    "    print(f\"Found {results.total} results:\")\n",
    "    for i, post in enumerate(results.docs):\n",
    "        score = 1 - float(post.vector_score)\n",
    "        print(f\"\\t{i}. {post.url} (Score: {round(score ,3) })\")\n",
    "else:\n",
    "    print(\"No results found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b1b0ab8-d6a3-43a3-93f7-2230267c0eff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MODULE 7 - EMBEDDING MODELS",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
